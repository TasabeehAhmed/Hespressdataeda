{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"ce8a994f-919b-4335-8478-0c75c6ca8b05","_cell_guid":"4eb54256-e09d-42bb-bf77-385619cb056f","collapsed":false,"execution":{"iopub.status.busy":"2023-07-24T22:50:29.525262Z","iopub.execute_input":"2023-07-24T22:50:29.526026Z","iopub.status.idle":"2023-07-24T22:50:29.583312Z","shell.execute_reply.started":"2023-07-24T22:50:29.525986Z","shell.execute_reply":"2023-07-24T22:50:29.581998Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom pandas import DataFrame\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.tokenize import WordPunctTokenizer\nfrom nltk.stem.isri import ISRIStemmer\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nimport itertools\nimport random\nimport os\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nimport re\nimport nltk\nimport sklearn\nfrom collections import Counter","metadata":{"_uuid":"4bd00807-05d7-47a0-8ffa-2b98a66248c6","_cell_guid":"275ed905-e84e-4b1b-a235-b0f590ad4722","collapsed":false,"execution":{"iopub.status.busy":"2023-07-24T22:50:29.585708Z","iopub.execute_input":"2023-07-24T22:50:29.586152Z","iopub.status.idle":"2023-07-24T22:50:32.563527Z","shell.execute_reply.started":"2023-07-24T22:50:29.586121Z","shell.execute_reply":"2023-07-24T22:50:32.562534Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folder_path = '/kaggle/input/hespress'  \n\ncsv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n\ndata_frames = {}\n\nfor csv_file in csv_files:\n    csv_file_path = os.path.join(folder_path, csv_file)\n    print(f\"Contents of {csv_file}:\")\n    df = pd.read_csv(csv_file_path)\n    df = df.iloc[:, 2:] \n    display(df)\n    print(\"\\n\")","metadata":{"_uuid":"d2a4b3e5-e15d-4e0d-95f6-af16a9111827","_cell_guid":"c0d561b4-9582-4fca-a65b-cde2af6c656a","collapsed":false,"execution":{"iopub.status.busy":"2023-07-24T22:50:32.566328Z","iopub.execute_input":"2023-07-24T22:50:32.566931Z","iopub.status.idle":"2023-07-24T22:50:40.698702Z","shell.execute_reply.started":"2023-07-24T22:50:32.566889Z","shell.execute_reply":"2023-07-24T22:50:40.697213Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessing_ar(text):\n    text = stemming_ar(text)\n    text = stopWordRemove_ar(text)\n    text = normlizeArabic_ar(text)\n    return text\n\ndef stopWordRemove_ar(text):\n    my_st_file = open(\"/kaggle/input/stopwords/allstop.txt\")\n    my_list = my_st_file.read()\n    needed_word = []\n    words = word_tokenize(text)\n    for w in words:\n        if w not in my_list:\n            needed_word.append(w)\n    filtered_sentence = ' '.join(needed_word)\n    return filtered_sentence\n\ndef stemming_ar(text):\n    st = ISRIStemmer()\n    stemmed_words = []\n    words = word_tokenize(text)\n    for w in words:\n        stemmed_words.append(st.stem(w))\n    stemmed_sentence = ' '.join(stemmed_words)\n    return stemmed_sentence\n\npunctuations = '''`1234567890÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ''' + string.punctuation\n\ndef normlizeArabic_ar(text):\n    translator = str.maketrans('', '', punctuations)\n    text = text.translate(translator)\n\n    text = re.sub(\"[إأآا]\", \"ا\", text)\n    text = re.sub(\"ى\", \"ي\", text)\n    text = re.sub(\"ؤ\", \"ء\", text)\n    text = re.sub(\"ئ\", \"ء\", text)\n    text = re.sub(\"ة\", \"ه\", text)\n    text = re.sub(\"گ\", \"ك\", text)\n\n    noise = re.compile(\"\"\"\n                             ّ    | # Shadda\n                             َ    | # Fatha\n                             ً    | # Tanwin Fath\n                             ُ    | # Damma\n                             ٌ    | # Tanwin Damm\n                             ِ    | # Kasra\n                             ٍ    | # Tanwin Kasr\n                             ْ    | # Sukun\n                             ـ     # Tatwil/Kashida\n                         \"\"\", re.VERBOSE)\n    text = re.sub(noise, \"\", text)\n\n    return text\n\ndef prepareDataSet(df):\n    if 'comment' in df.columns and 'story' in df.columns:\n        print(\"Before preprocessing - 'comment' column:\")\n        print(df['comment'])\n        print(\"\\nBefore preprocessing - 'story' column:\")\n        print(df['story'])\n        df['comment'] = df['comment'].apply(lambda x: x if isinstance(x, str) else '')\n        df['story'] = df['story'].apply(lambda x: x if isinstance(x, str) else '')\n        df['comment'] = df['comment'].apply(preprocessing_ar)\n        df['story'] = df['story'].apply(preprocessing_ar)\n    elif 'comment' in df.columns:\n        print(\"Before preprocessing - 'comment' column:\")\n        print(df['comment'])\n        df['comment'] = df['comment'].apply(lambda x: x if isinstance(x, str) else '')\n        df['comment'] = df['comment'].apply(preprocessing_ar)\n    elif 'story' in df.columns:\n        print(\"Before preprocessing - 'story' column:\")\n        print(df['story'])\n        df['story'] = df['story'].apply(lambda x: x if isinstance(x, str) else '')\n        df['story'] = df['story'].apply(preprocessing_ar)\n    else:\n        print(\"No 'comment' or 'story' column found in the DataFrame.\")\n    return df\n\ndef remove_non_strings(df):\n    for col in df.columns:\n        if col not in ['comment', 'story']:\n            continue\n        df[col] = df[col].apply(lambda x: x if isinstance(x, str) else '')\n    return df\n\nfolder_path = '/kaggle/input/hespress'  \n\ncsv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\npreprocessed_dfs = []\nfor csv_file in csv_files:\n    csv_file_path = os.path.join(folder_path, csv_file)\n    print(f\"Contents of {csv_file}:\")\n    df = pd.read_csv(csv_file_path)\n    df = df.iloc[:, 2:] \n    df = prepareDataSet(df)\n    df_cleaned = remove_non_strings(df)\n    preprocessed_dfs.append(df_cleaned)\n    display(df_cleaned)\n    print(\"\\n\")","metadata":{"_uuid":"ac65cd48-04b2-4f27-b548-e59c96dc33c7","_cell_guid":"0003e648-353c-4b6f-a6f6-794322e9f6a3","collapsed":false,"execution":{"iopub.status.busy":"2023-07-24T22:57:16.358497Z","iopub.execute_input":"2023-07-24T22:57:16.358997Z","iopub.status.idle":"2023-07-24T23:15:58.379100Z","shell.execute_reply.started":"2023-07-24T22:57:16.358957Z","shell.execute_reply":"2023-07-24T23:15:58.377505Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_and_visualize_eda_stories(df, story_col='story', topic_col='topic', author_col='author'):\n    # Check if the DataFrame contains required columns\n    if story_col in df.columns and topic_col in df.columns:\n        # Apply EDA specific to 'stories' DataFrame\n        print(\"EDA for Stories DataFrame:\")\n\n        # Number of examples per topic\n        examples_per_topic = df[topic_col].value_counts()\n\n        # Top frequent n-grams\n        n = 2  \n        all_texts = ' '.join(df[story_col])\n        all_ngrams = Counter(zip(*[all_texts[i:] for i in range(n)]))\n        top_ngrams_overall = all_ngrams.most_common(10)\n\n        class_ngrams = {}\n        for topic_name, topic_group in df.groupby(topic_col):\n            class_texts = ' '.join(topic_group[story_col])\n            class_ngrams[topic_name] = Counter(zip(*[class_texts[i:] for i in range(n)]))\n            class_top_ngrams = class_ngrams[topic_name].most_common(10)\n\n            # Print the top 10 most common n-grams per topic\n            print(f\"\\nTop Frequent {n}-grams for {topic_name}:\")\n            for ngram, count in class_top_ngrams:\n                print(f\"{ngram} - Count: {count}\")\n\n        # Lengths of examples in words and letters\n        df['word_count'] = df[story_col].apply(lambda x: len(x.split()))\n        df['letter_count'] = df[story_col].apply(lambda x: len(x.replace(' ', '')))\n\n        # Insights and Visualizations\n        print(\"Number of Examples per Topic:\")\n        print(examples_per_topic)\n\n        print(\"\\nTop Frequent {n}-grams Overall:\")\n        for ngram, count in top_ngrams_overall:\n            print(f\"{ngram} - Count: {count}\")\n\n        # Bar plot: Number of examples per topic\n        plt.figure(figsize=(8, 5))\n        sns.barplot(x=examples_per_topic.index, y=examples_per_topic.values)\n        plt.xlabel('Topic')\n        plt.ylabel('Number of Examples')\n        plt.title('Number of Examples per Topic (Stories)')\n        plt.xticks(rotation=45)\n        plt.show()\n\n        # Word count distribution\n        plt.figure(figsize=(8, 5))\n        sns.histplot(df['word_count'], kde=True)\n        plt.xlabel('Word Count')\n        plt.ylabel('Frequency')\n        plt.title('Distribution of Word Count (Stories)')\n        plt.show()\n\n        # Letter count distribution\n        plt.figure(figsize=(8, 5))\n        sns.histplot(df['letter_count'], kde=True)\n        plt.xlabel('Letter Count')\n        plt.ylabel('Frequency')\n        plt.title('Distribution of Letter Count (Stories)')\n        plt.show()\n        # Author Analysis (for stories dataset)\n        top_authors_stories = df[author_col].value_counts().nlargest(5)\n        print(\"\\nAuthor Analysis (Top 5 Authors - Stories):\")\n        print(top_authors_stories)\nfor df in preprocessed_dfs:\n    if 'story' in df.columns and 'topic' in df.columns:\n        calculate_and_visualize_eda_stories(df)","metadata":{"_uuid":"6db348cd-53a6-47c7-9df4-f5812b694a11","_cell_guid":"05b7fc72-4f2a-41bf-8e45-832f2358a3cc","collapsed":false,"execution":{"iopub.status.busy":"2023-07-25T00:50:52.651998Z","iopub.execute_input":"2023-07-25T00:50:52.652587Z","iopub.status.idle":"2023-07-25T00:51:14.195424Z","shell.execute_reply.started":"2023-07-25T00:50:52.652545Z","shell.execute_reply":"2023-07-25T00:51:14.193901Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom wordcloud import WordCloud\n\ndef calculate_and_visualize_eda_comments(df, comment_col='comment', topic_col='topic', score_col='score'):\n    # Check if the DataFrame contains required columns\n    if comment_col in df.columns and topic_col in df.columns and score_col in df.columns:\n        # Apply EDA specific to 'comments' DataFrame\n        print(f\"EDA for Comments DataFrame (CSV File: {df.name}):\")\n\n        # Number of examples per topic\n        examples_per_topic_comments = df[topic_col].value_counts()\n\n        # Convert 'score' column to numeric\n        df['score'] = pd.to_numeric(df['score'], errors='coerce')\n\n        # Sentiment Analysis (for comments dataset)\n        positive_comments = df[df['score'] >= 0]\n        negative_comments = df[df['score'] < 0]\n        sentiment_distribution_comments = pd.DataFrame({\n            'Sentiment': ['Positive', 'Negative'],\n            'Count': [len(positive_comments), len(negative_comments)]\n        })\n        print(\"\\nSentiment Analysis (Positive and Negative Comments):\")\n        print(sentiment_distribution_comments)\n\n        \nfor i, df in enumerate(preprocessed_dfs):\n    if 'comment' in df.columns:\n        \n        df.name = f\"DataFrame_{i}\"\n        calculate_and_visualize_eda_comments(df)","metadata":{"_uuid":"7f234a46-b24e-4190-9c8c-eb12305855e9","_cell_guid":"169cc0b2-a3c1-4065-a7ad-1638c375e35d","collapsed":false,"execution":{"iopub.status.busy":"2023-07-25T00:46:27.107391Z","iopub.execute_input":"2023-07-25T00:46:27.109072Z","iopub.status.idle":"2023-07-25T00:46:27.234922Z","shell.execute_reply.started":"2023-07-25T00:46:27.109009Z","shell.execute_reply":"2023-07-25T00:46:27.233443Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"c923698e-026d-49c5-9022-a003bab66fbd","_cell_guid":"97f12d29-06e2-44fb-8ae3-5d021a7625ff","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}